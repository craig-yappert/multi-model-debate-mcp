import * as vscode from 'vscode';
import { MCPClient } from '../mcp/client';
import { ConversationStore, ConversationEntry, ConversationThread, ThreadMessage, AIToAIRequest } from '../storage/conversation-store';
import { ContextAnalyzer, VSCodeContext } from './context-analyzer';
import { CommandLineManager } from './command-line-manager';
import { MattermostFallback } from './mattermost-fallback';

export class ChatParticipantManager {
    private mattermostFallback: MattermostFallback;

    constructor(
        private mcpClient: MCPClient,
        private conversationStore: ConversationStore,
        private contextAnalyzer: ContextAnalyzer,
        private commandLineManager: CommandLineManager
    ) {
        // Initialize Mattermost fallback with current configuration
        const config = vscode.workspace.getConfiguration('multiModelDebate');
        this.mattermostFallback = new MattermostFallback(config);
    }

    async handleChatRequest(
        persona: string,
        request: vscode.ChatRequest,
        context: vscode.ChatContext,
        stream: vscode.ChatResponseStream,
        token: vscode.CancellationToken
    ): Promise<{ metadata: { command: string } }> {

        // Note: VS Code's model selector in chat is for the built-in Copilot integration.
        // Our extension uses specific personas (@claude-research, @kiro, @copilot, @team)
        // that route to our MCP server, not the VS Code model selector.
        // The model shown in VS Code chat UI doesn't affect our persona routing.

        // Special handling for team persona
        if (persona === 'team') {
            return this.handleTeamRequest(request, context, stream, token);
        }

        const startTime = Date.now();

        try {
            // Show thinking indicator with clarification about routing
            stream.progress(`${this.getPersonaDisplayName(persona)} is analyzing your request...`);

            // If user seems confused about model selection, clarify
            if (request.prompt.toLowerCase().includes('gpt') || request.prompt.toLowerCase().includes('model')) {
                stream.markdown(`‚ÑπÔ∏è *Note: This extension uses specific AI personas (@claude-research, @kiro, @copilot) that connect through our MCP server, independent of VS Code's model selector.*\n\n`);
            }

            // Gather rich context from VS Code
            const codeContext = await this.contextAnalyzer.gatherContext();

            // Get conversation history for context
            const conversationHistory = await this.conversationStore.getRecentConversations(5);

            // Prepare the request with all context
            const enhancedRequest = {
                message: request.prompt,
                persona: persona,
                vscode_context: codeContext,
                conversation_history: conversationHistory,
                workspace: vscode.workspace.name,
                timestamp: new Date().toISOString()
            };

            stream.progress(`Connecting to AI personas...`);

            // Get AI response from MCP server
            const aiResponse = await this.mcpClient.contribute(enhancedRequest);

            if (!aiResponse || aiResponse.trim().length === 0) {
                throw new Error('No response received from AI persona');
            }

            // Log response length for debugging
            console.log(`Response from ${persona}: ${aiResponse.length} characters`);
            console.log('First 500 chars:', aiResponse.substring(0, 500));
            console.log('Last 500 chars:', aiResponse.substring(Math.max(0, aiResponse.length - 500)));

            // Check if response contains command line suggestions
            const commandSuggestions = this.extractCommandSuggestions(aiResponse);

            // Stream the response
            stream.progress(`${this.getPersonaDisplayName(persona)} is responding...`);

            // Add persona header with icon
            stream.markdown(`**${this.getPersonaDisplayName(persona)}** ${this.getPersonaIcon(persona)}\n\n`);

            // Stream the main response
            await this.streamResponse(stream, aiResponse);

            // Handle command suggestions if present
            if (commandSuggestions.length > 0) {
                await this.handleCommandSuggestions(stream, commandSuggestions);
            }

            // Save conversation to workspace storage
            await this.conversationStore.saveConversation({
                id: '', // Will be generated by saveConversation
                persona,
                message: request.prompt,
                response: aiResponse,
                context: codeContext,
                timestamp: new Date().toISOString(),
                responseTime: Date.now() - startTime
            });

            // Add metadata about file context for VS Code
            const metadata = {
                command: `ai-response-${persona}`,
                fileContext: codeContext.activeFile?.path || 'none',
                hasErrors: codeContext.diagnostics.length > 0,
                isDebugging: codeContext.debugging?.isActive || false
            };

            return { metadata };

        } catch (error) {
            console.error(`Error in ${persona} chat participant:`, error);

            // Fallback to Mattermost or error response
            const fallbackResponse = await this.handleFallbackResponse(persona, request.prompt, error);
            stream.markdown(`**${this.getPersonaDisplayName(persona)}** ‚ö†Ô∏è\n\n${fallbackResponse}`);

            // Show fallback action buttons
            await this.mattermostFallback.handleFallbackActions(stream);

            // Prompt fallback configuration if needed
            await this.mattermostFallback.promptFallbackConfiguration();

            return { metadata: { command: `ai-error-${persona}` } };
        }
    }

    private async streamResponse(stream: vscode.ChatResponseStream, response: string): Promise<void> {
        // Get configuration
        const config = vscode.workspace.getConfiguration('multiModelDebate');
        const useChunking = config.get<boolean>('useResponseChunking', true);

        if (!useChunking) {
            // Send entire response at once to avoid truncation
            stream.markdown(response);
            console.log(`Sent entire response at once: ${response.length} characters`);
        } else {
            // Use chunking if enabled
            const chunkSize = config.get<number>('responseChunkSize', 5000);
            const streamDelay = config.get<number>('responseStreamDelay', 5);

            // Stream response in configurable chunks
            const chunks = this.chunkResponse(response, chunkSize);
            console.log(`Streaming ${chunks.length} chunks of max size ${chunkSize}`);

            for (const chunk of chunks) {
                stream.markdown(chunk);
                // Configurable delay between chunks
                if (streamDelay > 0) {
                    await new Promise(resolve => setTimeout(resolve, streamDelay));
                }
            }
        }
    }

    private chunkResponse(text: string, chunkSize: number): string[] {
        const chunks: string[] = [];
        let currentChunk = '';
        const words = text.split(' ');

        for (const word of words) {
            if ((currentChunk + ' ' + word).length > chunkSize && currentChunk.length > 0) {
                chunks.push(currentChunk);
                currentChunk = word;
            } else {
                currentChunk = currentChunk ? `${currentChunk} ${word}` : word;
            }
        }

        if (currentChunk) {
            chunks.push(currentChunk);
        }

        return chunks;
    }

    private extractCommandSuggestions(response: string): string[] {
        // Look for command suggestions in the response
        const commandPatterns = [
            /```(?:bash|shell|cmd)\n([\s\S]*?)\n```/g,
            /`([^`]+)`/g
        ];

        const suggestions: string[] = [];

        for (const pattern of commandPatterns) {
            let match;
            while ((match = pattern.exec(response)) !== null) {
                const command = match[1].trim();
                if (this.looksLikeCommand(command)) {
                    suggestions.push(command);
                }
            }
        }

        return [...new Set(suggestions)]; // Remove duplicates
    }

    private looksLikeCommand(text: string): boolean {
        // Simple heuristics to identify commands
        const commandIndicators = [
            'npm ', 'yarn ', 'git ', 'python ', 'node ',
            'cd ', 'ls ', 'mkdir ', 'rm ', 'cp ', 'mv ',
            'pip ', 'cargo ', 'go ', 'docker ', 'kubectl '
        ];

        return commandIndicators.some(indicator => text.toLowerCase().startsWith(indicator));
    }

    private async handleCommandSuggestions(stream: vscode.ChatResponseStream, suggestions: string[]): Promise<void> {
        if (!this.commandLineManager.isEnabled()) {
            return;
        }

        stream.markdown('\n\n---\n\n**Command Suggestions:**\n\n');

        for (const suggestion of suggestions) {
            // Create a button for each command suggestion
            stream.button({
                command: 'multiModelDebate.executeCommand',
                title: `‚ñ∂Ô∏è Run: ${suggestion}`,
                arguments: [suggestion]
            });
        }
    }

    private async handleFallbackResponse(persona: string, message: string, error: any): Promise<string> {
        // Use the dedicated Mattermost fallback handler
        return this.mattermostFallback.getFallbackResponse(persona, error);
    }

    private getPersonaDisplayName(persona: string): string {
        switch (persona) {
            case 'claude-research':
                return 'Claude Research';
            case 'kiro':
                return 'Kiro';
            case 'copilot':
                return 'GitHub Copilot';
            case 'team':
                return 'AI Team';
            default:
                return persona;
        }
    }

    private getPersonaIcon(persona: string): string {
        switch (persona) {
            case 'claude-research':
                return 'üß†'; // Brain for analytical thinking
            case 'kiro':
                return 'üîß'; // Tools for practical execution
            case 'copilot':
                return 'üöÅ'; // Helicopter for oversight and integration
            case 'team':
                return 'ü§ù'; // Handshake for collaboration
            default:
                return 'ü§ñ';
        }
    }

    async handleTeamRequest(
        request: vscode.ChatRequest,
        context: vscode.ChatContext,
        stream: vscode.ChatResponseStream,
        token: vscode.CancellationToken
    ): Promise<{ metadata: { command: string } }> {
        const startTime = Date.now();

        try {
            // Parse team command
            const prompt = request.prompt.trim();
            const commandMatch = prompt.match(/^(debate|collaborate|discuss)\s+(.+)$/);
            
            let command = 'collaborate';
            let topic = prompt;

            if (commandMatch) {
                command = commandMatch[1];
                topic = commandMatch[2];
            }

            stream.progress('ü§ù Team is initializing multi-AI collaboration...');

            // Create a new conversation thread
            const participants = ['claude-research', 'kiro', 'copilot'];
            const thread = await this.conversationStore.createThread(participants, topic);

            stream.markdown(`**AI Team** ü§ù\n\n`);
            stream.markdown(`Starting ${command} session: **${topic}**\n\n`);
            stream.markdown(`Participants: @claude-research üß†, @kiro üîß, and @copilot ÔøΩ\n\n`);
            stream.markdown(`---\n\n`);

            // Gather context for the conversation
            const codeContext = await this.contextAnalyzer.gatherContext();
            const conversationHistory = await this.conversationStore.getRecentConversations(3);

            // Start the multi-agent conversation
            await this.orchestrateMultiAgentConversation(
                thread, 
                topic, 
                command, 
                stream, 
                codeContext, 
                conversationHistory
            );

            // Update thread status
            await this.conversationStore.updateThreadStatus(thread.id, 'concluded');

            stream.markdown(`\n---\n\n**üéØ Team Collaboration Complete**\n\n`);

            return { 
                metadata: { 
                    command: `team-${command}`
                } 
            };

        } catch (error) {
            console.error('Error in team chat participant:', error);
            stream.markdown(`**AI Team** ‚ö†Ô∏è\n\nError during team collaboration: ${error}`);
            return { metadata: { command: 'team-error' } };
        }
    }

    private async orchestrateMultiAgentConversation(
        thread: ConversationThread,
        topic: string,
        command: string,
        stream: vscode.ChatResponseStream,
        codeContext: VSCodeContext,
        conversationHistory: ConversationEntry[]
    ): Promise<void> {
        const maxTurns = 4; // Prevent infinite loops
        let currentSpeaker = 'claude-research'; // Start with research
        let turn = 0;

        // Initial prompt to get the conversation started
        let currentMessage = this.getInitialPrompt(command, topic, codeContext);

        while (turn < maxTurns) {
            turn++;

            stream.progress(`Turn ${turn}: @${currentSpeaker} is thinking...`);

            try {
                // Create AI-to-AI request
                const aiRequest: AIToAIRequest = {
                    fromPersona: turn === 1 ? 'team' : this.getOtherPersona(currentSpeaker),
                    toPersona: currentSpeaker,
                    message: currentMessage,
                    conversationContext: conversationHistory,
                    interactionType: this.getInteractionType(turn, command),
                    threadId: thread.id
                };

                // Get response from current AI
                const response = await this.mcpClient.sendAIToAIMessage(aiRequest);

                // Display the response
                stream.markdown(`**@${currentSpeaker}** ${this.getPersonaIcon(currentSpeaker)}\n\n`);
                await this.streamResponse(stream, response);
                stream.markdown(`\n\n`);

                // Add message to thread
                const threadMessage: ThreadMessage = {
                    id: `msg-${Date.now()}-${Math.random().toString(36).substr(2, 9)}`,
                    persona: currentSpeaker,
                    message: response,
                    timestamp: new Date().toISOString(),
                    interactionType: aiRequest.interactionType
                };

                await this.conversationStore.addMessageToThread(thread.id, threadMessage);

                // Prepare message for next AI
                currentMessage = this.createFollowUpPrompt(response, command, turn);

                // Switch to other persona
                currentSpeaker = this.getOtherPersona(currentSpeaker);

                // Check if conversation should conclude
                if (this.shouldConcludeConversation(response, turn)) {
                    break;
                }

            } catch (error) {
                console.error(`Error in turn ${turn} with ${currentSpeaker}:`, error);
                stream.markdown(`‚ö†Ô∏è Error communicating with @${currentSpeaker}: ${error}\n\n`);
                break;
            }
        }

        // Generate final synthesis
        await this.generateSynthesis(thread, stream);
    }

    private getInitialPrompt(command: string, topic: string, context: VSCodeContext): string {
        const contextSummary = this.summarizeContext(context);
        
        switch (command) {
            case 'debate':
                return `Let's have a constructive debate about: ${topic}\n\nCurrent context: ${contextSummary}\n\nPlease present your initial perspective and reasoning.`;
            case 'collaborate':
                return `Let's collaborate on: ${topic}\n\nCurrent context: ${contextSummary}\n\nPlease share your initial thoughts and proposed approach.`;
            case 'discuss':
                return `Let's discuss: ${topic}\n\nCurrent context: ${contextSummary}\n\nPlease provide your analysis and insights.`;
            default:
                return `Let's work together on: ${topic}\n\nCurrent context: ${contextSummary}\n\nPlease share your perspective.`;
        }
    }

    private createFollowUpPrompt(previousResponse: string, command: string, turn: number): string {
        const responsePrefix = `Previous response from colleague: "${previousResponse}"\n\n`;
        
        if (turn >= 3) {
            return responsePrefix + "Please provide your final thoughts and help synthesize our discussion into actionable conclusions.";
        }

        switch (command) {
            case 'debate':
                return responsePrefix + "Please respond with your counterpoints, alternative perspectives, or build upon the valid points raised.";
            case 'collaborate':
                return responsePrefix + "Please build upon this idea, suggest improvements, or identify implementation considerations.";
            default:
                return responsePrefix + "Please respond with your thoughts, questions, or additional insights.";
        }
    }

    private getInteractionType(turn: number, command: string): 'response' | 'challenge' | 'build_upon' | 'synthesize' {
        if (turn >= 3) return 'synthesize';
        if (command === 'debate') return turn % 2 === 0 ? 'challenge' : 'response';
        return 'build_upon';
    }

    private getOtherPersona(currentPersona: string): string {
        return currentPersona === 'claude-research' ? 'kiro' : 'claude-research';
    }

    private shouldConcludeConversation(response: string, turn: number): boolean {
        // Simple heuristics to detect conclusion
        const conclusionWords = ['conclusion', 'finally', 'in summary', 'to summarize', 'overall'];
        const hasConclusion = conclusionWords.some(word => 
            response.toLowerCase().includes(word)
        );
        
        return turn >= 3 && hasConclusion;
    }

    private async generateSynthesis(thread: ConversationThread, stream: vscode.ChatResponseStream): Promise<void> {
        stream.progress('Generating synthesis of the conversation...');

        try {
            const messages = thread.messages.map(msg => 
                `${msg.persona}: ${msg.message}`
            ).join('\n\n');

            const synthesisRequest: AIToAIRequest = {
                fromPersona: 'team',
                toPersona: 'claude-research',
                message: `Please create a synthesis of this multi-AI conversation:\n\n${messages}\n\nProvide a summary of key points, areas of agreement, disagreements, and actionable conclusions.`,
                conversationContext: [],
                interactionType: 'synthesize',
                threadId: thread.id
            };

            const synthesis = await this.mcpClient.sendAIToAIMessage(synthesisRequest);

            stream.markdown(`**üéØ Conversation Synthesis**\n\n`);
            stream.markdown(synthesis);

        } catch (error) {
            console.error('Error generating synthesis:', error);
            stream.markdown(`**üéØ Conversation Synthesis**\n\n‚ö†Ô∏è Could not generate synthesis: ${error}`);
        }
    }

    private summarizeContext(context: VSCodeContext): string {
        const parts = [];
        
        if (context.activeFile) {
            parts.push(`Active file: ${context.activeFile.path}`);
        }
        
        if (context.workspace?.name) {
            parts.push(`Workspace: ${context.workspace.name}`);
        }

        if (context.diagnostics.length > 0) {
            parts.push(`${context.diagnostics.length} diagnostic issues`);
        }

        if (context.git?.branch) {
            parts.push(`Git branch: ${context.git.branch}`);
        }

        return parts.length > 0 ? parts.join(', ') : 'No specific context';
    }
}